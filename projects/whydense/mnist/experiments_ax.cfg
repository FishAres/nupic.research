# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
# with Numenta, Inc., for a separate license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

#
# Parameters used in the paper
#

[DEFAULT]
path = ~/nta/results/mnist
verbose = 2
checkpoint_freq = 1
checkpoint_at_end = False

; Uncomment to save results to S3
;upload_dir = "s3://bucketname/ray/whydense/mnist"
;sync_function = "aws s3 sync `dirname {local_dir}` {remote_dir}/`basename $(dirname {local_dir})`"

; Uncomment to average over multiple seeds
;repetitions = 1
;seed = 42
repetitions = 10
seed = tune.sample_from(lambda spec: np.random.randint(1, 10000))

# Set to 'True' to save/restore the model on every iteration and repetition
restore_supported = False

data_dir = ~/nta/data/
no_cuda = False
log_interval = 2000

# Common network parameters
weight_sparsity = (0.3, )
cnn_weight_sparsity = (1.0, )
use_batch_norm = False
boost_strength = 1.5
boost_strength_factor = 0.85
k_inference_factor = 1.0

# Common training regime / optimizer parameters
iterations = 15
validation = 1.0
optimizer = SGD
lr_scheduler = StepLR
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"
dropout = 0.0
batches_in_epoch = 100000
batch_size = 64
first_epoch_batch_size = %(batch_size)s
batches_in_first_epoch = %(batches_in_epoch)s
test_noise_every_epoch = False
test_batch_size = 1000
learning_rate = 0.01
learning_rate_factor = 0.8
learning_schedule_step_size = 1
momentum = 0.0

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters trained using Ax hyperparameter tuning

[denseCNN2]
cnn_out_channels = (32, 64)
cnn_percent_on = (1.0, 1.0)
cnn_weight_sparsity = (1.0, 1.0)
linear_n = (700,)
linear_percent_on = (1.0,)
weight_sparsity = (1.0, )
boost_strength = 0.0
learning_rate = 0.038
learning_rate_factor = 0.97
learning_schedule_step_size = 4
momentum = 0.766
weight_decay = 1.73e-5
