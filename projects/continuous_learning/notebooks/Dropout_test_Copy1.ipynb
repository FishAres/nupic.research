{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from cont_speech_experiment import ContinuousSpeechExperiment, ClasswiseDataset\n",
    "from nupic.research.support import parse_config\n",
    "\n",
    "from nupic.research.frameworks.continuous_learning.utils import (\n",
    "    clear_labels,\n",
    "    freeze_output_layer,\n",
    "    split_inds,\n",
    ")\n",
    "    \n",
    "from exp_lesparse import LeSparseNet\n",
    "\n",
    "import os\n",
    "\n",
    "from nupic.research.frameworks.pytorch.model_utils import evaluate_model\n",
    "from nupic.research.frameworks.continuous_learning.dendrite_layers import (\n",
    "    DendriteLayer, DendriteInput, DendriteOutput\n",
    ")\n",
    "from nupic.torch.modules import (\n",
    "    Flatten,\n",
    "    KWinners,\n",
    "    KWinners2d,\n",
    "    SparseWeights,\n",
    "    SparseWeights2d,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimizer with learning rate= 0.01\n"
     ]
    }
   ],
   "source": [
    "config_file = \"../experiments.cfg\"\n",
    "with open(config_file) as cf:\n",
    "    config_init = parse_config(cf)\n",
    "    \n",
    "exp = \"sparseCNN2\"\n",
    "\n",
    "config = config_init[exp]\n",
    "config[\"name\"] = exp\n",
    "config[\"use_dendrites\"] = False\n",
    "config[\"use_batch_norm\"] = True\n",
    "config[\"cnn_out_channels\"] = (256, 256)\n",
    "config[\"cnn_percent_on\"] = (1., 1.) # (0.2, 0.2)\n",
    "config[\"cnn_weight_sparsity\"] = (1., 1.) #(0.5, 0.5)\n",
    "config[\"dendrites_per_cell\"] = 2\n",
    "config[\"batch_size\"] = 64\n",
    "# config[\"momentum\"] = 0.0\n",
    "experiment = ContinuousSpeechExperiment(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules, indices = [], []\n",
    "module_sizes = [256, 256, 1000, 11]\n",
    "cnt = 0\n",
    "for module in experiment.model:\n",
    "    if hasattr(module, \"weight_sparsity\"):\n",
    "        inds = split_inds(module, 5, module_sizes[cnt])\n",
    "        final_inds = [np.delete(np.arange(module_sizes[cnt]), kidx) for kidx in inds]\n",
    "        modules.append(module)\n",
    "        indices.append(final_inds)\n",
    "        \n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds = np.random.permutation(np.arange(1,11)).reshape(5,2)\n",
    "for j in range(len(train_inds)):\n",
    "    temp_inds = [k[j] for k in indices]\n",
    "    experiment.train(1, train_inds[j], freeze_modules=modules[:-1], module_inds=temp_inds[:-1],\n",
    "                    freeze_output=True, output_indices=clear_labels(train_inds[j]),\n",
    "                     layer_type=\"kwinner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[experiment.test_class(k)[\"mean_accuracy\"] for k in train_inds.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2523cc290>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(experiment.running_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([  1,   2,   3,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "          15,  16,  17,  19,  20,  23,  24,  25,  26,  27,  28,  30,  31,\n",
       "          32,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,\n",
       "          48,  49,  50,  52,  55,  56,  58,  59,  60,  61,  63,  64,  65,\n",
       "          67,  69,  70,  71,  73,  74,  75,  77,  78,  79,  80,  81,  82,\n",
       "          83,  84,  85,  86,  87,  89,  90,  92,  93,  94,  95,  96,  98,\n",
       "         100, 101, 102, 104, 105, 106, 109, 110, 111, 112, 113, 114, 115,\n",
       "         116, 117, 118, 120, 122, 123, 126, 127, 129, 130, 131, 132, 133,\n",
       "         134, 135, 136, 137, 138, 139, 141, 143, 144, 145, 146, 147, 148,\n",
       "         150, 151, 152, 154, 155, 156, 158, 159, 160, 161, 162, 163, 165,\n",
       "         166, 168, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180,\n",
       "         183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 197,\n",
       "         198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
       "         212, 214, 215, 216, 217, 219, 220, 221, 222, 225, 226, 227, 228,\n",
       "         229, 230, 231, 232, 233, 234, 235, 237, 238, 240, 241, 243, 244,\n",
       "         245, 246, 247, 248, 249, 250, 251, 252, 254, 255]),\n",
       "  array([  0,   1,   2,   4,   5,   6,   7,  11,  12,  13,  14,  15,  16,\n",
       "          18,  20,  21,  22,  24,  27,  28,  29,  30,  31,  32,  33,  34,\n",
       "          35,  36,  37,  38,  40,  41,  43,  45,  46,  47,  48,  49,  51,\n",
       "          53,  54,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,\n",
       "          67,  68,  69,  70,  72,  74,  76,  79,  81,  83,  84,  85,  86,\n",
       "          87,  88,  89,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100,\n",
       "         101, 102, 103, 104, 105, 106, 107, 108, 109, 112, 114, 116, 117,\n",
       "         119, 120, 121, 122, 123, 124, 125, 128, 129, 131, 133, 134, 135,\n",
       "         136, 137, 138, 139, 140, 142, 143, 144, 146, 147, 149, 151, 152,\n",
       "         153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166,\n",
       "         167, 168, 169, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "         182, 183, 184, 185, 187, 189, 190, 191, 192, 194, 195, 196, 200,\n",
       "         201, 202, 203, 204, 205, 206, 207, 209, 210, 212, 213, 214, 215,\n",
       "         217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229,\n",
       "         230, 231, 232, 233, 234, 236, 237, 238, 239, 241, 242, 243, 244,\n",
       "         245, 246, 247, 249, 250, 251, 252, 253, 254, 255]),\n",
       "  array([  0,   2,   3,   4,   5,   6,   8,   9,  10,  11,  12,  13,  16,\n",
       "          17,  18,  19,  20,  21,  22,  23,  25,  26,  27,  28,  29,  30,\n",
       "          31,  32,  33,  34,  36,  37,  38,  39,  40,  41,  42,  43,  44,\n",
       "          46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  57,  58,  59,\n",
       "          60,  62,  63,  64,  65,  66,  67,  68,  70,  71,  72,  73,  74,\n",
       "          75,  76,  77,  78,  79,  80,  81,  82,  87,  88,  89,  90,  91,\n",
       "          92,  94,  97,  98,  99, 101, 102, 103, 104, 107, 108, 109, 110,\n",
       "         111, 113, 115, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128,\n",
       "         129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
       "         142, 143, 144, 145, 146, 148, 149, 150, 151, 153, 155, 156, 157,\n",
       "         159, 161, 162, 163, 164, 165, 167, 168, 170, 171, 172, 173, 175,\n",
       "         176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
       "         189, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203,\n",
       "         206, 207, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
       "         220, 221, 222, 223, 224, 225, 226, 230, 233, 234, 235, 236, 237,\n",
       "         239, 240, 241, 242, 243, 246, 248, 250, 253, 254]),\n",
       "  array([  0,   1,   2,   3,   4,   6,   7,   8,   9,  10,  12,  13,  14,\n",
       "          15,  17,  18,  19,  21,  22,  23,  24,  25,  26,  27,  29,  30,\n",
       "          32,  33,  35,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,\n",
       "          50,  51,  52,  53,  54,  55,  56,  57,  58,  60,  61,  62,  64,\n",
       "          65,  66,  67,  68,  69,  70,  71,  72,  73,  75,  76,  77,  78,\n",
       "          79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "          93,  95,  96,  97,  98,  99, 100, 103, 104, 105, 106, 107, 108,\n",
       "         109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
       "         122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 135, 137,\n",
       "         138, 139, 140, 141, 142, 144, 145, 147, 148, 149, 150, 151, 152,\n",
       "         153, 154, 156, 157, 158, 160, 161, 164, 165, 166, 167, 169, 170,\n",
       "         173, 174, 175, 177, 178, 180, 181, 182, 183, 184, 186, 187, 188,\n",
       "         189, 190, 191, 193, 194, 196, 197, 198, 199, 200, 201, 202, 204,\n",
       "         205, 208, 209, 211, 213, 216, 218, 219, 221, 222, 223, 224, 227,\n",
       "         228, 229, 230, 231, 232, 233, 235, 236, 238, 239, 240, 242, 243,\n",
       "         244, 245, 246, 247, 248, 249, 251, 252, 253, 255]),\n",
       "  array([  0,   1,   3,   4,   5,   7,   8,   9,  10,  11,  14,  15,  16,\n",
       "          17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  28,  29,  31,\n",
       "          33,  34,  35,  36,  37,  39,  42,  44,  45,  46,  47,  48,  49,\n",
       "          50,  51,  52,  53,  54,  55,  56,  57,  59,  61,  62,  63,  66,\n",
       "          68,  69,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  82,\n",
       "          83,  84,  85,  86,  88,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          99, 100, 101, 102, 103, 105, 106, 107, 108, 110, 111, 112, 113,\n",
       "         114, 115, 116, 117, 118, 119, 121, 123, 124, 125, 126, 127, 128,\n",
       "         130, 132, 133, 134, 136, 140, 141, 142, 143, 145, 146, 147, 148,\n",
       "         149, 150, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163,\n",
       "         164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 179,\n",
       "         181, 182, 185, 186, 188, 189, 190, 191, 192, 193, 195, 196, 197,\n",
       "         198, 199, 200, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
       "         213, 214, 215, 216, 217, 218, 220, 223, 224, 225, 226, 227, 228,\n",
       "         229, 231, 232, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244,\n",
       "         245, 247, 248, 249, 250, 251, 252, 253, 254, 255]),\n",
       "  array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "          13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "          26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "          39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "          52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "          65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "          78,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "          92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "         105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "         118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "         131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "         144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "         157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "         170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "         183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "         209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "         222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "         235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "         248, 249, 250, 251, 252, 253, 254, 255])],\n",
       " [array([  0,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  13,  17,\n",
       "          18,  19,  23,  24,  25,  26,  27,  29,  30,  31,  32,  33,  34,\n",
       "          35,  37,  38,  39,  40,  41,  44,  45,  46,  47,  50,  51,  52,\n",
       "          53,  54,  55,  56,  57,  58,  59,  61,  63,  64,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  76,  78,  79,  80,  82,  83,  84,  86,\n",
       "          87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,\n",
       "         100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
       "         115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129,\n",
       "         130, 131, 132, 133, 134, 137, 138, 139, 140, 141, 142, 143, 144,\n",
       "         145, 146, 147, 148, 149, 151, 152, 153, 154, 156, 157, 158, 159,\n",
       "         161, 162, 163, 164, 165, 166, 167, 169, 170, 172, 175, 176, 177,\n",
       "         178, 179, 181, 183, 184, 186, 187, 189, 190, 191, 192, 193, 195,\n",
       "         198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211,\n",
       "         213, 214, 215, 216, 217, 218, 221, 224, 225, 227, 228, 229, 230,\n",
       "         231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243,\n",
       "         244, 245, 246, 247, 249, 250, 251, 252, 253, 254]),\n",
       "  array([  0,   1,   2,   3,   4,   5,   6,  10,  12,  13,  14,  15,  16,\n",
       "          17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,\n",
       "          31,  32,  34,  36,  37,  38,  42,  43,  44,  45,  46,  47,  48,\n",
       "          49,  51,  52,  53,  54,  56,  58,  59,  60,  61,  62,  63,  65,\n",
       "          66,  67,  68,  70,  71,  72,  73,  74,  75,  77,  78,  79,  80,\n",
       "          81,  83,  84,  85,  86,  87,  88,  90,  91,  92,  93,  94,  95,\n",
       "          96,  98, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 140,\n",
       "         141, 143, 146, 147, 150, 151, 153, 154, 155, 156, 157, 159, 160,\n",
       "         161, 163, 164, 166, 167, 168, 171, 172, 173, 174, 175, 176, 177,\n",
       "         178, 179, 180, 182, 183, 184, 185, 187, 188, 189, 191, 192, 193,\n",
       "         194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 206, 207, 208,\n",
       "         209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 222, 223,\n",
       "         226, 227, 228, 231, 232, 233, 234, 235, 237, 238, 239, 241, 242,\n",
       "         243, 245, 246, 247, 248, 249, 251, 252, 253, 255]),\n",
       "  array([  0,   1,   2,   5,   7,   8,   9,  10,  11,  12,  13,  14,  15,\n",
       "          16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  28,  29,\n",
       "          30,  32,  33,  35,  36,  38,  39,  40,  41,  42,  43,  44,  45,\n",
       "          46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  57,  59,  60,\n",
       "          61,  62,  64,  65,  66,  67,  68,  69,  71,  72,  73,  75,  76,\n",
       "          77,  79,  80,  81,  82,  83,  84,  85,  87,  88,  89,  90,  92,\n",
       "          94,  95,  97,  98,  99, 100, 101, 102, 104, 105, 106, 107, 109,\n",
       "         110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 126,\n",
       "         127, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 141, 142,\n",
       "         143, 144, 145, 146, 147, 148, 149, 150, 152, 155, 156, 158, 159,\n",
       "         160, 161, 162, 163, 165, 166, 168, 169, 170, 171, 172, 173, 174,\n",
       "         176, 177, 179, 180, 181, 182, 185, 186, 188, 189, 190, 191, 192,\n",
       "         193, 194, 195, 196, 197, 199, 200, 202, 203, 204, 205, 208, 210,\n",
       "         212, 213, 216, 217, 218, 219, 220, 222, 223, 224, 225, 226, 228,\n",
       "         229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243,\n",
       "         244, 245, 247, 248, 249, 250, 251, 252, 254, 255]),\n",
       "  array([  0,   1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  20,  21,  22,  25,  27,  28,  30,  31,  32,  33,\n",
       "          34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  45,  47,  48,\n",
       "          49,  50,  51,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
       "          64,  65,  66,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "          81,  82,  85,  86,  89,  91,  93,  94,  95,  96,  97,  99, 100,\n",
       "         101, 102, 103, 104, 108, 109, 110, 111, 113, 114, 115, 116, 117,\n",
       "         118, 120, 122, 123, 124, 125, 126, 127, 128, 131, 132, 134, 135,\n",
       "         136, 139, 140, 141, 142, 143, 144, 145, 148, 149, 150, 151, 152,\n",
       "         153, 154, 155, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167,\n",
       "         168, 169, 170, 171, 172, 173, 174, 175, 177, 178, 180, 181, 182,\n",
       "         183, 184, 185, 186, 187, 188, 190, 191, 192, 194, 196, 197, 198,\n",
       "         199, 200, 201, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
       "         214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
       "         227, 228, 229, 230, 231, 235, 236, 237, 238, 240, 241, 242, 243,\n",
       "         244, 245, 246, 248, 250, 251, 252, 253, 254, 255]),\n",
       "  array([  1,   2,   3,   4,   6,   7,   8,   9,  11,  12,  14,  15,  16,\n",
       "          17,  18,  19,  20,  21,  22,  23,  24,  26,  27,  28,  29,  30,\n",
       "          31,  33,  34,  35,  36,  37,  39,  40,  41,  42,  43,  44,  46,\n",
       "          48,  49,  50,  52,  53,  55,  56,  57,  58,  60,  62,  63,  64,\n",
       "          65,  66,  67,  68,  69,  70,  74,  75,  76,  77,  78,  79,  80,\n",
       "          81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
       "          96,  97,  98,  99, 100, 101, 102, 103, 105, 106, 107, 108, 111,\n",
       "         112, 116, 119, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131,\n",
       "         133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146,\n",
       "         147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160,\n",
       "         161, 162, 164, 165, 167, 168, 169, 170, 171, 173, 174, 175, 176,\n",
       "         178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190,\n",
       "         193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206,\n",
       "         207, 209, 211, 212, 214, 215, 217, 218, 219, 220, 221, 222, 223,\n",
       "         224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 236, 237, 239,\n",
       "         240, 244, 246, 247, 248, 249, 250, 253, 254, 255]),\n",
       "  array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "          13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "          26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "          39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "          52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "          65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "          78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "          91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "         104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "         117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "         130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "         144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "         157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "         170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "         183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "         209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "         222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "         235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "         248, 249, 250, 251, 252, 253, 254, 255])]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0'),\n",
       " tensor(0., device='cuda:0')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 0\n",
    "a,b = list(modules[ind].named_parameters())\n",
    "[a[1].grad[k,...].mean() for k in temp_inds[ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 795 is out of bounds for dimension 0 with size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-976a486c03d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_inds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-976a486c03d4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_inds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 795 is out of bounds for dimension 0 with size 128"
     ]
    }
   ],
   "source": [
    "w = list(experiment.model.named_parameters())\n",
    "[w[2][1].grad[k,...].mean() for k in temp_inds[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fbff40bbad0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_grads(module, inds):\n",
    "    with torch.no_grad():\n",
    "        weight_grads, bias_grads = list(module.parameters())\n",
    "        if len(weight_grads.shape) > 2:\n",
    "            [weight_grads.data[index,:,:,:].fill_(0.0) for index in inds]\n",
    "        else:\n",
    "            [weight_grads.data[index,:].fill_(0.0) for index in inds]\n",
    "        [bias_grads.data[index].fill_(0.0) for index in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    freeze_modules=None,\n",
    "    freeze_splits=5,\n",
    "    freeze_output=False,\n",
    "    layer_type=\"dense\",\n",
    "    linear_number=2,\n",
    "    output_indices=None,\n",
    "    criterion=F.nll_loss,\n",
    "    batches_in_epoch=sys.maxsize,\n",
    "    pre_batch_callback=None,\n",
    "    post_batch_callback=None,\n",
    "    progress_bar=None,\n",
    "    combine_data=False,\n",
    "):\n",
    "\n",
    "    model.train()\n",
    "    # Use asynchronous GPU copies when the memory is pinned\n",
    "    # See https://pytorch.org/docs/master/notes/cuda.html\n",
    "    async_gpu = loader.pin_memory\n",
    "    if progress_bar is not None:\n",
    "        loader = tqdm(loader, **progress_bar)\n",
    "        # update progress bar total based on batches_in_epoch\n",
    "        if batches_in_epoch < len(loader):\n",
    "            loader.total = batches_in_epoch\n",
    "\n",
    "    # Check if training with Apex Mixed Precision\n",
    "    use_amp = hasattr(optimizer, \"_amp_stash\")\n",
    "    try:\n",
    "        from apex import amp\n",
    "    except ImportError:\n",
    "        if use_amp:\n",
    "            raise ImportError(\n",
    "                \"Mixed precision requires NVIDA APEX.\"\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        if batch_idx >= batches_in_epoch:\n",
    "            break\n",
    "\n",
    "        num_images = len(target)\n",
    "        data = data.to(device, non_blocking=async_gpu)\n",
    "        target = target.to(device, non_blocking=async_gpu)\n",
    "        t1 = time.time()\n",
    "\n",
    "        if pre_batch_callback is not None:\n",
    "            pre_batch_callback(model=model, batch_idx=batch_idx)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if combine_data:\n",
    "            output = model(data, target)\n",
    "        else:\n",
    "            output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        del data, target, output\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        if freeze_modules is not None:\n",
    "            for mod_ in freeze_modules:\n",
    "                module, inds = get_inds(mod_, freeze_splits)\n",
    "                freeze_grads(module, inds)\n",
    "\n",
    "        if freeze_output:\n",
    "            freeze_output_layer(model, output_indices, layer_type=layer_type,\n",
    "                                linear_number=linear_number)\n",
    "\n",
    "        t3 = time.time()\n",
    "        optimizer.step() # step\n",
    "        t4 = time.time()\n",
    "\n",
    "        if post_batch_callback is not None:\n",
    "            time_string = (\"Data: {:.3f}s, forward: {:.3f}s, backward: {:.3f}s,\"\n",
    "                           + \"weight update: {:.3f}s\").format(t1 - t0, t2 - t1, t3 - t2,\n",
    "                                                              t4 - t3)\n",
    "            post_batch_callback(model=model, loss=loss.detach(), batch_idx=batch_idx,\n",
    "                                num_images=num_images, time_string=time_string)\n",
    "        del loss\n",
    "        t0 = time.time()\n",
    "\n",
    "    if progress_bar is not None:\n",
    "        loader.n = loader.total\n",
    "        loader.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeSparseNet(\n",
       "  (cnn1_cnn): SparseWeights2d(\n",
       "    sparsity=0.85\n",
       "    (module): Conv2d(1, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (cnn1_kwinner): KWinners2d(channels=128, local=False, n=0, percent_on=0.12, boost_strength=1.5, boost_strength_factor=0.9, k_inference_factor=1.0, duty_cycle_period=1000)\n",
       "  (cnn1_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2_cnn): SparseWeights2d(\n",
       "    sparsity=0.95\n",
       "    (module): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (cnn2_kwinner): KWinners2d(channels=128, local=False, n=0, percent_on=0.07, boost_strength=1.5, boost_strength_factor=0.9, k_inference_factor=1.0, duty_cycle_period=1000)\n",
       "  (cnn2_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten()\n",
       "  (linear1): SparseWeights(\n",
       "    sparsity=0.8\n",
       "    (module): Linear(in_features=3200, out_features=1000, bias=True)\n",
       "  )\n",
       "  (linear1_kwinners): KWinners(n=1000, percent_on=0.1, boost_strength=1.5, boost_strength_factor=0.9, k_inference_factor=1.0, duty_cycle_period=1000)\n",
       "  (linear2): SparseWeights(\n",
       "    sparsity=0.8\n",
       "    (module): Linear(in_features=1000, out_features=11, bias=True)\n",
       "  )\n",
       "  (linear2_kwinners): KWinners(n=11, percent_on=0.2, boost_strength=1.5, boost_strength_factor=0.9, k_inference_factor=1.0, duty_cycle_period=1000)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 3, 4, 9, 5, 6, 8, 2, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
