{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from cont_speech_experiment import ContinuousSpeechExperiment\n",
    "from nupic.research.frameworks.backprop_structure.modules.binary_layers import BinaryGatedConv2d, BinaryGatedLinear\n",
    "from nupic.torch.modules import (\n",
    "    Flatten,\n",
    "    KWinners,\n",
    "    KWinners2d,\n",
    "    SparseWeights,\n",
    "    SparseWeights2d,\n",
    ")\n",
    "\n",
    "from nupic.research.support.parse_config import parse_config\n",
    "from nupic.research.frameworks.continuous_learning.utils import train_model, clear_labels\n",
    "from nupic.research.frameworks.pytorch.model_utils import evaluate_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimizer with learning rate= 0.01\n"
     ]
    }
   ],
   "source": [
    "config_file = \"../experiments.cfg\"\n",
    "with open(config_file) as cf:\n",
    "    config_init = parse_config(cf)\n",
    "    \n",
    "exp = \"sparseCNN2\"\n",
    "\n",
    "config = config_init[exp]\n",
    "config[\"name\"] = exp\n",
    "\n",
    "experiment = ContinuousSpeechExperiment(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (1, 32, 32)\n",
    "cnn_channels = (128, 64)\n",
    "kernel_size=5\n",
    "\n",
    "m = BinaryGatedConv2d(in_channels=1,\n",
    "                     out_channels=cnn_channels[0],\n",
    "                     kernel_size=kernel_size).cuda()\n",
    "\n",
    "k1 = KWinners2d(cnn_channels[0],\n",
    "               percent_on=0.1,).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_size=(1,32,32),\n",
    "                n_classes=11,\n",
    "                cnn_channels=(64, 64),\n",
    "                linear_n=(1000,),\n",
    "                cnn_droprate_init=(0.5,0.5),\n",
    "                linear_droprate_init=(0.5, 0.5),\n",
    "                l0_strength =(0.5, 0.5),\n",
    "                l2_strength=(0.5, 0.5),\n",
    "                cnn_pct_on=(0.1,0.1),\n",
    "                linear_pct_on=(0.1,),\n",
    "                boost_strength=(1., 1.),\n",
    "                boost_strength_factor=(0.9, 0.9),\n",
    "                duty_cycle_period=(1000, 1000),\n",
    "                batch_norm=True,\n",
    "                ):\n",
    "        super(ToyNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.cnn_channels = cnn_channels\n",
    "        self.linear_n = linear_n\n",
    "        self.cnn_pct_on = cnn_pct_on\n",
    "        self.linear_pct_on = linear_pct_on\n",
    "        self.boost_strength = boost_strength\n",
    "        self.boost_strength_factor = boost_strength_factor\n",
    "        self.duty_cycle_period = duty_cycle_period\n",
    "        self.batch_norm = batch_norm\n",
    "    \n",
    "        self.cnn1 = BinaryGatedConv2d(in_channels=input_size[0],\n",
    "                                     out_channels=cnn_channels[0],\n",
    "                                     kernel_size=5,\n",
    "                                     droprate_init=cnn_droprate_init[0],\n",
    "                                     l0_strength=l0_strength[0],\n",
    "                                     l2_strength=l2_strength[0],\n",
    "                                     )\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(cnn_channels[0], affine=False)\n",
    "        \n",
    "        self.mp1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.k1 = KWinners2d(channels=cnn_channels[0],\n",
    "                            percent_on=cnn_pct_on[0],\n",
    "                            boost_strength=boost_strength[0],\n",
    "                            boost_strength_factor=boost_strength_factor[0],\n",
    "                            duty_cycle_period=duty_cycle_period[0],)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.linear1 = BinaryGatedLinear(in_features=self.conv_out(cnn_channels[0]), \n",
    "                                        out_features=linear_n[0],\n",
    "                                        droprate_init=linear_droprate_init[0],\n",
    "                                        l0_strength=l0_strength[1],\n",
    "                                        l2_strength=l2_strength[1],\n",
    "                                    )\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(linear_n[0], affine=False)\n",
    "        \n",
    "        self.linear1_k = KWinners(n=linear_n[0], \n",
    "                                 percent_on=linear_pct_on[0],\n",
    "                                 boost_strength=boost_strength[1], # -- NOTE -- replace this if you add 2nd conv layer\n",
    "                                 boost_strength_factor=boost_strength_factor[1],\n",
    "                                 duty_cycle_period=duty_cycle_period[1],        \n",
    "                                 )\n",
    "        \n",
    "        self.output = nn.Linear(linear_n[0], n_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv component\n",
    "        x = self.cnn1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.mp1(x)\n",
    "        x = self.k1(x)\n",
    "        # flatten\n",
    "        x = self.flatten(x)\n",
    "        # Linear component\n",
    "        x = self.linear1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.linear1_k(x)\n",
    "        \n",
    "        # output\n",
    "        x = self.output(x)\n",
    "        x = self.log_softmax(x)\n",
    "        \n",
    "        return x\n",
    "                                         \n",
    "    def conv_out(self, cnn_channels):\n",
    "        if cnn_channels == 64:\n",
    "            out_size = 12544\n",
    "        elif cnn_channels == 256:\n",
    "            out_size = 50176\n",
    "        return out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "net = ToyNet(n_classes=num_classes,\n",
    "            cnn_channels=(256,256),\n",
    "            linear_n=(2048,),\n",
    "            cnn_droprate_init=(0.1,),\n",
    "            linear_droprate_init=(0.1,),\n",
    "            boost_strength=(0., 0.),\n",
    "            boost_strength_factor=(0., 0.),\n",
    "            duty_cycle_period=(20000, 20000),\n",
    "            ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac_train: 0.60, l_0 reg: 1.00, l_2 reg: 1.00\n",
      "[0, 0.0, 0.01, 0.58, 0.44]\n",
      "frac_train: 0.60, l_0 reg: 1.00, l_2 reg: 1.00\n",
      "[0, 0.02, 0.02, 0.73, 0.22]\n",
      "frac_train: 0.60, l_0 reg: 1.00, l_2 reg: 0.50\n",
      "[0, 0.46, 0.06, 0.05, 0.11]\n",
      "frac_train: 0.60, l_0 reg: 1.00, l_2 reg: 0.50\n",
      "[0, 0.41, 0.02, 0.1, 0.19]\n",
      "frac_train: 0.60, l_0 reg: 1.00, l_2 reg: 0.20\n",
      "[0, 0.01, 0.06, 0.27, 0.06]\n",
      "frac_train: 0.60, l_0 reg: 1.00, l_2 reg: 0.20\n",
      "[0, 0.07, 0.12, 0.24, 0.04]\n",
      "frac_train: 0.60, l_0 reg: 0.50, l_2 reg: 1.00\n",
      "[0, 0.14, 0.19, 0.35, 0.04]\n",
      "frac_train: 0.60, l_0 reg: 0.50, l_2 reg: 1.00\n",
      "[0, 0.2, 0.16, 0.19, 0.07]\n",
      "frac_train: 0.60, l_0 reg: 0.50, l_2 reg: 0.50\n",
      "[0, 0.46, 0.06, 0.18, 0.04]\n",
      "frac_train: 0.60, l_0 reg: 0.50, l_2 reg: 0.50\n",
      "[0, 0.42, 0.08, 0.32, 0.03]\n",
      "frac_train: 0.60, l_0 reg: 0.50, l_2 reg: 0.20\n",
      "[0, 0.17, 0.09, 0.0, 0.26]\n",
      "frac_train: 0.60, l_0 reg: 0.50, l_2 reg: 0.20\n",
      "[0, 0.25, 0.17, 0.0, 0.12]\n",
      "frac_train: 0.60, l_0 reg: 0.20, l_2 reg: 1.00\n",
      "[0, 0.12, 0.39, 0.34, 0.15]\n",
      "frac_train: 0.60, l_0 reg: 0.20, l_2 reg: 1.00\n",
      "[0, 0.02, 0.33, 0.2, 0.37]\n",
      "frac_train: 0.60, l_0 reg: 0.20, l_2 reg: 0.50\n",
      "[0, 0.04, 0.13, 0.18, 0.46]\n",
      "frac_train: 0.60, l_0 reg: 0.20, l_2 reg: 0.50\n",
      "[0, 0.07, 0.08, 0.09, 0.57]\n",
      "frac_train: 0.60, l_0 reg: 0.20, l_2 reg: 0.20\n",
      "[0, 0.55, 0.13, 0.0, 0.25]\n",
      "frac_train: 0.60, l_0 reg: 0.20, l_2 reg: 0.20\n",
      "[0, 0.42, 0.03, 0.0, 0.3]\n",
      "frac_train: 0.50, l_0 reg: 1.00, l_2 reg: 1.00\n",
      "[0, 0.48, 0.08, 0.0, 0.02]\n",
      "frac_train: 0.50, l_0 reg: 1.00, l_2 reg: 1.00\n",
      "[0, 0.51, 0.04, 0.0, 0.05]\n",
      "frac_train: 0.50, l_0 reg: 1.00, l_2 reg: 0.50\n",
      "[0, 0.11, 0.34, 0.1, 0.21]\n",
      "frac_train: 0.50, l_0 reg: 1.00, l_2 reg: 0.50\n",
      "[0, 0.05, 0.46, 0.09, 0.17]\n",
      "frac_train: 0.50, l_0 reg: 1.00, l_2 reg: 0.20\n",
      "[0, 0.18, 0.25, 0.0, 0.03]\n",
      "frac_train: 0.50, l_0 reg: 1.00, l_2 reg: 0.20\n",
      "[0, 0.12, 0.06, 0.0, 0.07]\n",
      "frac_train: 0.50, l_0 reg: 0.50, l_2 reg: 1.00\n",
      "[0, 0.06, 0.14, 0.09, 0.12]\n",
      "frac_train: 0.50, l_0 reg: 0.50, l_2 reg: 1.00\n",
      "[0, 0.04, 0.08, 0.06, 0.09]\n",
      "frac_train: 0.50, l_0 reg: 0.50, l_2 reg: 0.50\n",
      "[0, 0.17, 0.14, 0.18, 0.16]\n",
      "frac_train: 0.50, l_0 reg: 0.50, l_2 reg: 0.50\n",
      "[0, 0.22, 0.36, 0.09, 0.14]\n",
      "frac_train: 0.50, l_0 reg: 0.50, l_2 reg: 0.20\n",
      "[0, 0.02, 0.52, 0.07, 0.05]\n",
      "frac_train: 0.50, l_0 reg: 0.50, l_2 reg: 0.20\n",
      "[0, 0.0, 0.42, 0.07, 0.24]\n",
      "frac_train: 0.50, l_0 reg: 0.20, l_2 reg: 1.00\n",
      "[0, 0.23, 0.14, 0.32, 0.23]\n",
      "frac_train: 0.50, l_0 reg: 0.20, l_2 reg: 1.00\n",
      "[0, 0.12, 0.06, 0.26, 0.4]\n",
      "frac_train: 0.50, l_0 reg: 0.20, l_2 reg: 0.50\n",
      "[0, 0.62, 0.05, 0.04, 0.23]\n",
      "frac_train: 0.50, l_0 reg: 0.20, l_2 reg: 0.50\n",
      "[0, 0.51, 0.1, 0.04, 0.29]\n",
      "frac_train: 0.50, l_0 reg: 0.20, l_2 reg: 0.20\n",
      "[0, 0.3, 0.0, 0.0, 0.65]\n",
      "frac_train: 0.50, l_0 reg: 0.20, l_2 reg: 0.20\n",
      "[0, 0.22, 0.08, 0.01, 0.63]\n",
      "frac_train: 0.40, l_0 reg: 1.00, l_2 reg: 1.00\n",
      "[0, 0.38, 0.09, 0.12, 0.25]\n",
      "frac_train: 0.40, l_0 reg: 1.00, l_2 reg: 1.00\n",
      "[0, 0.47, 0.02, 0.11, 0.34]\n",
      "frac_train: 0.40, l_0 reg: 1.00, l_2 reg: 0.50\n",
      "[0, 0.08, 0.29, 0.56, 0.0]\n",
      "frac_train: 0.40, l_0 reg: 1.00, l_2 reg: 0.50\n",
      "[0, 0.05, 0.22, 0.63, 0.02]\n",
      "frac_train: 0.40, l_0 reg: 1.00, l_2 reg: 0.20\n",
      "[0, 0.18, 0.37, 0.39, 0.09]\n",
      "frac_train: 0.40, l_0 reg: 1.00, l_2 reg: 0.20\n",
      "[0, 0.2, 0.25, 0.23, 0.17]\n",
      "frac_train: 0.40, l_0 reg: 0.50, l_2 reg: 1.00\n",
      "[0, 0.06, 0.5, 0.26, 0.15]\n",
      "frac_train: 0.40, l_0 reg: 0.50, l_2 reg: 1.00\n",
      "[0, 0.05, 0.59, 0.17, 0.12]\n"
     ]
    }
   ],
   "source": [
    "class_inds = np.arange(1,5).reshape(2,2) # [1, 2], [3, 4]\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for frac_batches in np.arange(0.6, 0.05, -0.1):\n",
    "    for l0 in (1., 0.5, 0.2):\n",
    "        for l2 in (1., 0.5, 0.2):\n",
    "    \n",
    "            net = ToyNet(n_classes=num_classes,\n",
    "                    cnn_channels=(256,256),\n",
    "                    linear_n=(2048,),\n",
    "                    cnn_droprate_init=(0.1,),\n",
    "                    linear_droprate_init=(0.1,),\n",
    "                    l0_strength=(l0,l0),\n",
    "                    l2_strength=(l2,l2),\n",
    "                    boost_strength=(0., 0.),\n",
    "                    boost_strength_factor=(0., 0.),\n",
    "                    duty_cycle_period=(20000, 20000),\n",
    "                    ).cuda()\n",
    "        \n",
    "            for j in range(len(class_inds)):\n",
    "                experiment.combine_classes(class_inds[j])\n",
    "                train_model(net, experiment.train_loader, optimizer,\n",
    "                           device=torch.device(\"cuda\"), sample_fraction=frac_batches,\n",
    "                           freeze_output=True, layer_type=\"dense\",\n",
    "                           output_indices=clear_labels(class_inds[j], length=num_classes)\n",
    "                           )\n",
    "                accs = [np.round(test_model(net, class_loader),2)\n",
    "                        for class_loader in experiment.test_loader[:num_classes]]\n",
    "\n",
    "                print(\"frac_train: {:.2f}, l_0 reg: {:.2f}, l_2 reg: {:.2f}\".format(frac_batches, l0, l2))\n",
    "                print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20482"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.dataset.tensors[1].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little helper functions to avoid crowding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader):\n",
    "    tst = evaluate_model(model,\n",
    "                        loader,\n",
    "                        device=torch.device(\"cuda\"),\n",
    "                        )\n",
    "    return tst[\"mean_accuracy\"]\n",
    "\n",
    "def get_example(loader=None):\n",
    "    if loader is None:\n",
    "        loader = experiment.full_train_loader\n",
    "    x, y = next(iter(loader))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNetBackpropStructure(input_size=(1, 32, 32),\n",
    "                             num_classes=11,\n",
    "                            droprate_init=0.1).cuda()\n",
    "\n",
    "loader = experiment.full_train_loader\n",
    "test_loader = experiment.gen_test_loader\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5184748427672956\n",
      "0.4669811320754717\n",
      "0.6344339622641509\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train_model(net, loader, optimizer, device=torch.device(\"cuda\"), normalize_input=False)\n",
    "    res = evaluate_model(net, test_loader, device=torch.device(\"cuda\"))\n",
    "    print(res[\"mean_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_correct': 1740,\n",
       " 'total_tested': 2544,\n",
       " 'mean_loss': -2332.2959905660377,\n",
       " 'mean_accuracy': 0.6839622641509434}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(net, test_loader, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = experiment.full_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20482,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.dataset.tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
